{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and custom modules\n",
    "import sklearn\n",
    "sklearn.set_config(transform_output=\"pandas\")\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from rapidfuzz import process, fuzz\n",
    "from geopy.distance import geodesic\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "\n",
    "\n",
    "import survival.utils\n",
    "import survival.constants\n",
    "importlib.reload(survival.utils)\n",
    "importlib.reload(survival.constants)\n",
    "from survival.utils import show_all\n",
    "from survival.utils import validate_imputation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_parquet(\"../../data/processed/raw_clean.parquet\")\n",
    "hh = pd.read_csv(\"../../data/processed/hh_clean.csv\")\n",
    "cities = pd.read_csv('../../data/processed/cities.csv')\n",
    "nl = pd.read_csv('../../data/processed/nl.csv')\n",
    "countries = pd.read_csv('../../data/processed/countries.csv')\n",
    "income = pd.read_csv('../../data/processed/income.csv')\n",
    "gemeenten = pd.read_csv('../../data/processed/gemeenten.csv')\n",
    "postalcode = pd.read_csv('../../data/processed/postalcode.csv')\n",
    "reg = pd.read_csv('../../data/processed/operaballet_reg_prods_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_date = pd.Timestamp('2025-05-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_purchase_dates_agg = data.groupby(['id', 'start_date'], observed=True)['purchase_date'].transform('min')\n",
    "\n",
    "data['min_purchase_date'] = min_purchase_dates_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per id per start_date, count the amount of tickets bought and store in column 'order_size'\n",
    "# count per ticket type the amount of tickets bought and store in columns 'order_size_<ticket_type>'. \n",
    "# fill with 0 if no tickets bought\n",
    "data['order_size'] = data.groupby(['id', 'start_date'])['id'].transform('count')\n",
    "data['total_order_value'] = data.groupby(['id', 'start_date'])['price'].transform('sum')\n",
    "data['avg_order_value'] = data.groupby(['id', 'start_date'])['price'].transform('mean')\n",
    "data['total_order_value'] = data['total_order_value'].round(2)\n",
    "data['avg_order_value'] = data['avg_order_value'].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop these\n",
    "# but perhaps drop ticket_num earlier -> figure out if its necessary in a grouping operation\n",
    "data = data.drop(columns=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sales = data.groupby(['start_date', 'production', 'min_purchase_date']).size().reset_index(name='tickets_sold')\n",
    "    \n",
    "# cumulative sales for each performance\n",
    "perf_sales_cumulative = daily_sales.sort_values(['start_date', 'min_purchase_date'])\n",
    "perf_sales_cumulative['perf_sales_cumulative'] = perf_sales_cumulative.groupby('start_date')['tickets_sold'].cumsum()\n",
    "\n",
    "# cumulative sales for each production\n",
    "prod_sales_cumulative = daily_sales.sort_values(['production', 'min_purchase_date'])\n",
    "prod_sales_cumulative['prod_sales_cumulative'] = prod_sales_cumulative.groupby('production')['tickets_sold'].cumsum()\n",
    "\n",
    "# daily sales for each performance \n",
    "perf_sales_daily = daily_sales.groupby(['start_date', 'min_purchase_date'])['tickets_sold'].sum().reset_index(name='daily_perf_sales')\n",
    "\n",
    "# daily sales for each production \n",
    "prod_sales_daily = daily_sales.groupby(['production', 'min_purchase_date'])['tickets_sold'].sum().reset_index(name='daily_prod_sales')\n",
    "\n",
    "perf_to_merge = perf_sales_cumulative[['start_date', 'min_purchase_date', 'perf_sales_cumulative']].drop_duplicates(\n",
    "    subset=['start_date', 'min_purchase_date'], keep='last'\n",
    ")\n",
    "prod_to_merge = prod_sales_cumulative[['production', 'min_purchase_date', 'prod_sales_cumulative']].drop_duplicates(\n",
    "    subset=['production', 'min_purchase_date'], keep='last'\n",
    ")\n",
    "\n",
    "# final sales totals for each performance\n",
    "final_perf_sales = daily_sales.groupby('start_date')['tickets_sold'].sum().reset_index(name='final_perf_sales')\n",
    "\n",
    "data = data.merge(\n",
    "    perf_to_merge,\n",
    "    on=['start_date', 'min_purchase_date'],\n",
    "    how='left'\n",
    ")\n",
    "data = data.merge(\n",
    "    prod_to_merge,\n",
    "    on=['production', 'min_purchase_date'],\n",
    "    how='left'\n",
    ")\n",
    "data = data.merge(\n",
    "    perf_sales_daily[['start_date', 'min_purchase_date', 'daily_perf_sales']],\n",
    "    on=['start_date', 'min_purchase_date'],\n",
    "    how='left'\n",
    ")\n",
    "data = data.merge(\n",
    "    prod_sales_daily[['production', 'min_purchase_date', 'daily_prod_sales']],\n",
    "    on=['production', 'min_purchase_date'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "data = data.merge(\n",
    "    final_perf_sales, \n",
    "    on='start_date', \n",
    "    how='left'\n",
    "    )\n",
    "\n",
    "# sales percentage (how much of final sales occurred by purchase date)\n",
    "data['perf_sales_percentage'] = data['perf_sales_cumulative'] / data['final_perf_sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg['sale_start_date'] = pd.to_datetime(reg['sale_start_date'], errors='coerce')\n",
    "\n",
    "# merge sale start dates from reg to main data\n",
    "reg_lookup = reg[['production', 'sale_start_date']].drop_duplicates(subset=['production'])\n",
    "\n",
    "# if sale_start_date already exists in data\n",
    "has_sale_start_date = 'sale_start_date' in data.columns\n",
    "\n",
    "# merge with appropriate suffix \n",
    "if has_sale_start_date:\n",
    "    data = pd.merge(data, reg_lookup, on='production', how='left', suffixes=('', '_reg'))\n",
    "    # if column exists after merge, use to fill nans in the main column\n",
    "    if 'sale_start_date_reg' in data.columns:\n",
    "        data['sale_start_date'] = data['sale_start_date'].fillna(data['sale_start_date_reg'])\n",
    "        data = data.drop(columns=['sale_start_date_reg'])\n",
    "else:\n",
    "    data = pd.merge(data, reg_lookup, on='production', how='left')\n",
    "\n",
    "# productions with unknown start dates, find earliest purchase date\n",
    "missing_start_prods = [\n",
    "    '22/23 messa da requiem flirt', '22/23 forsythe flirt',\n",
    "    '23/24 die zauberflote flirt', '23/24 frida flirt', '23/24 gala hnb',\n",
    "    '23/24 dansers van morgen', '24/25 don quichot flirt'\n",
    "]\n",
    "\n",
    "# get earliest purchase date for each production needing imputation\n",
    "mask = data['production'].isin(missing_start_prods) & data['sale_start_date'].isna()\n",
    "if mask.any():\n",
    "    # group by production and get min purchase date\n",
    "    imputed_dates = data[mask].groupby('production')['min_purchase_date'].min()\n",
    "    \n",
    "    # apply imputed dates where needed\n",
    "    for prod, min_date in imputed_dates.items():\n",
    "        data.loc[(data['production'] == prod) & data['sale_start_date'].isna(), 'sale_start_date'] = min_date\n",
    "\n",
    "# date columns to datetime\n",
    "data['min_purchase_date'] = pd.to_datetime(data['min_purchase_date'], errors='coerce')\n",
    "data['sale_start_date'] = pd.to_datetime(data['sale_start_date'], errors='coerce') \n",
    "data['start_date'] = pd.to_datetime(data['start_date'], errors='coerce')\n",
    "\n",
    "# create features\n",
    "data['days_since_sale_start'] = (data['min_purchase_date'] - data['sale_start_date']).dt.days\n",
    "data['sale_period_duration_days'] = (data['start_date'] - data['sale_start_date']).dt.days\n",
    "\n",
    "# remove negative days since sale start\n",
    "data = data[data['days_since_sale_start'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the last performance date for each season\n",
    "season_end_dates = data.groupby('season')['start_date'].max().reset_index()\n",
    "season_end_dates.rename(columns={'start_date': 'season_last_performance'}, inplace=True)\n",
    "\n",
    "# merge back to main data\n",
    "data = data.merge(season_end_dates, on='season', how='left')\n",
    "\n",
    "# days left in season from performance date to last performance\n",
    "data['days_left_in_season'] = (data['season_last_performance'] - data['start_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subscription tickets\n",
    "subscription_ticket = ['abo standaard', 'abo vk dno', 'abonnement 22/23', 'kassa abo standaard',\n",
    "                       'abonnement 24/25', 'abo vk hnb', 'abo vrij', 'serie 2025-2026']\n",
    "\n",
    "# subscriber tickets\n",
    "data['is_subscriber'] = data['ticket_type'].isin(subscription_ticket).astype(int)\n",
    "\n",
    "# subscriber tickets per performance\n",
    "subscriber_count = data[data['is_subscriber'] == 1].groupby('start_date')['order_size'].sum().reset_index(name='subscriber_tickets')\n",
    "\n",
    "# merge subscriber count\n",
    "data = data.merge(subscriber_count, on='start_date', how='left')\n",
    "\n",
    "# nan values (performances with no subscribers)\n",
    "data['subscriber_tickets'] = data['subscriber_tickets'].fillna(0)\n",
    "\n",
    "# percentages\n",
    "data['subscriber_percentage'] = data['subscriber_tickets'] / 1633 \n",
    "data['regular_tickets'] = data['final_perf_sales'] - data['subscriber_tickets']\n",
    "data['regular_percentage'] = data['regular_tickets'] / 1633\n",
    "data['occupancy_rate'] = data['final_perf_sales'] / 1633\n",
    "\n",
    "# remove all subscription tickets from activity\n",
    "data = data[~data['ticket_type'].isin(subscription_ticket)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free tickets\n",
    "free_tickets = data[data['is_free'] == 1].groupby(['start_date', 'min_purchase_date']).size().reset_index(name='free_tickets')\n",
    "\n",
    "# Cumulative free tickets for each performance\n",
    "perf_free_tickets_cumulative = free_tickets.sort_values(['start_date', 'min_purchase_date'])\n",
    "perf_free_tickets_cumulative['perf_free_tickets_cumulative'] = perf_free_tickets_cumulative.groupby('start_date')['free_tickets'].cumsum()\n",
    "\n",
    "# operational discounts\n",
    "operational_discount = data.loc[data['ticket_type'].isin(['huiskorting', 'medewerker no&b', 'tell a friend']), \n",
    "                           ['start_date', 'min_purchase_date']].groupby(['start_date', 'min_purchase_date']).size().reset_index(name='operational_discount')\n",
    "\n",
    "# Cumulative company discount for each performance\n",
    "perf_operational_discount_cumulative = operational_discount.sort_values(['start_date', 'min_purchase_date'])\n",
    "perf_operational_discount_cumulative['perf_operational_discount_cumulative'] = perf_operational_discount_cumulative.groupby('start_date')['operational_discount'].cumsum()\n",
    "\n",
    "# Merge free tickets data\n",
    "data = data.merge(\n",
    "    perf_free_tickets_cumulative[['start_date', 'min_purchase_date', 'perf_free_tickets_cumulative']],\n",
    "    on=['start_date', 'min_purchase_date'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge company discount data\n",
    "data = data.merge(\n",
    "    perf_operational_discount_cumulative[['start_date', 'min_purchase_date', 'perf_operational_discount_cumulative']],\n",
    "    on=['start_date', 'min_purchase_date'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values with 0\n",
    "data['perf_free_tickets_cumulative'] = data['perf_free_tickets_cumulative'].fillna(0)\n",
    "data['perf_operational_discount_cumulative'] = data['perf_operational_discount_cumulative'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete records where is_free == 1 and drop the column\n",
    "data = data[data['is_free'] != 1]\n",
    "data = data.drop('is_free', axis=1)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all educatie tickets because these visitors are not unique\n",
    "data = data[~data['ticket_type'].str.contains('educatie')]\n",
    "\n",
    "# drop all ticket where ticket_type are related to employees\n",
    "irregular_ticket = [\n",
    "    'zoekplaats',\n",
    "    'huiskorting',\n",
    "    'medewerker',\n",
    "    'medewerker no&b',\n",
    "    'vrijplaats',\n",
    "    'paniek',\n",
    "    'balletorkest',\n",
    "    'orkest',\n",
    "    'nedpho',\n",
    "    'ckv leerling',\n",
    "    'ckv docent',\n",
    "    'streaming mia',\n",
    "    'groep -5%',\n",
    "    'groep -10%'\n",
    "    ]\n",
    "\n",
    "data = data[~data['ticket_type'].isin(irregular_ticket)]\n",
    "\n",
    "# drop all records where email contains @operaballet.nl\n",
    "data = data[data['email'].notna() & ~data['email'].str.contains('@operaballet.nl', na=False)]\n",
    "\n",
    "# drop the following ids because they are related to employees, institutions or groups\n",
    "from survival.constants import nonvisitor_ids\n",
    "data = data[~data['id'].isin(nonvisitor_ids)]\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidate ticket type names using ticket_categorie_mapping_dict\n",
    "from survival.constants import ticket_category_mapping_dict\n",
    "# flatten the values in ticket_category_mapping_dict\n",
    "flattened_ticket_categories = {item for sublist in ticket_category_mapping_dict.values() for item in sublist}\n",
    "# which values are in data[ticket_type].unique() that are not in the ticket_category_mapping_dict values\n",
    "missing_ticket_types = set(data['ticket_type'].unique()) - flattened_ticket_categories\n",
    "missing_ticket_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if a ticket_type name matches a value in the dict, replace it with the key\n",
    "# map ticket types to categories\n",
    "data['ticket_type'] = data['ticket_type'].map({ticket: category for category, tickets in ticket_category_mapping_dict.items() for ticket in tickets}).fillna('other')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank dictionaries\n",
    "ballet_rank_replace_dict = {\n",
    "    'premium': 1, 'rang 1': 2, 'rang 2': 3, 'rang 3': 4,\n",
    "    'rang 4': 5, 'rang 5': 6, 'rang 6': 7\n",
    "}\n",
    "opera_rank_replace_dict = {\n",
    "    'rang 1': 1, 'rang 2': 2, 'rang 3': 3, 'rang 4': 4,\n",
    "    'rang 5': 5, 'rang 6': 6, 'rang 7': 7\n",
    "}\n",
    "\n",
    "# dummies for artform\n",
    "data = pd.get_dummies(data, columns=['artform'], prefix='artform', drop_first=False)\n",
    "rename_dict = {}\n",
    "if 'artform_ballet' in data.columns: rename_dict['artform_ballet'] = 'ballet'\n",
    "if 'artform_opera' in data.columns: rename_dict['artform_opera'] = 'opera'\n",
    "if rename_dict: data = data.rename(columns=rename_dict)\n",
    "if 'ballet' in data.columns: data['ballet'] = data['ballet'].astype(int)\n",
    "if 'opera' in data.columns: data['opera'] = data['opera'].astype(int)\n",
    "\n",
    "\n",
    "# numeric_rank column with nan\n",
    "data['numeric_rank'] = np.nan \n",
    "\n",
    "# map ballet\n",
    "mask_ballet = (data['ballet'] == 1)\n",
    "# .map() for direct replacement\n",
    "# ranks not in the dict will become nan by default with .map()\n",
    "data.loc[mask_ballet, 'numeric_rank'] = data.loc[mask_ballet, 'rank'].map(ballet_rank_replace_dict)\n",
    "\n",
    "# map opera ranks \n",
    "# only for rows where numeric_rank is still nan to avoid overwriting ballet if an artform could be both\n",
    "mask_opera = (data['opera'] == 1) & (data['numeric_rank'].isna()) # Check if it could be both ballet and opera\n",
    "data.loc[mask_opera, 'numeric_rank'] = data.loc[mask_opera, 'rank'].map(opera_rank_replace_dict)\n",
    "\n",
    "\n",
    "# best rank per purchase\n",
    "best_rank_agg = data.groupby(['id', 'start_date'], observed=True).agg(\n",
    "    best_rank_per_purchase=('numeric_rank', 'min') # 'min' as lower number is better\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "# merge back best rank\n",
    "data = data.merge(best_rank_agg, on=['id', 'start_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the best_rank_per_purchase na's where [data['season'].isin(['2022_2023', '2023_2024', '2024_2025', '2025_2026']\n",
    "data = data[~((data['season'].isin(['2022_2023', '2023_2024', '2024_2025', '2025_2026'])) & (data['best_rank_per_purchase'].isna()))].copy()\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(\n",
    "    data.groupby(['id', 'start_date', 'ticket_type'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .add_prefix('tickets_type_'), \n",
    "    on=['id', 'start_date']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop ticket_type column\n",
    "data = data.drop(columns='ticket_type')\n",
    "\n",
    "# group by id and start_date and remove duplicates\n",
    "data = data.drop_duplicates(subset=['id', 'start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove out orders with more than 10 tickets as these are groups\n",
    "data = data[data['order_size'] <= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['total_order_value'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create next_purchase_date and time columns\n",
    "data = data.sort_values(by=['id', 'min_purchase_date'], ascending=True)\n",
    "data['next_purchase_date'] = data.groupby('id')['min_purchase_date'].shift(-1)\n",
    "data = data.drop_duplicates(subset=['id'], keep='first')\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left censoring\n",
    "# pre-attendance repurchasers\n",
    "pre_attendance_repurchasers = data[\n",
    "    (data['next_purchase_date'].notna()) &\n",
    "    (data['next_purchase_date'] <= data['start_date'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~((data['next_purchase_date'].notna()) & \n",
    "                       (data['next_purchase_date'] <= data['start_date']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only records where the performance has already occurred\n",
    "data = data[data['start_date'] <= retrieval_date]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lead days feature\n",
    "data['lead_days'] = (data['start_date'] - data['min_purchase_date']).dt.days\n",
    "\n",
    "# retain only lead days that are 0 or above\n",
    "data = data[data['lead_days'] >= 0]\n",
    "data = data.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age at time of purchase\n",
    "data['age_at_purchase'] = (data['min_purchase_date'] - data['birthdate']).dt.days / 365.25\n",
    "data['age_at_purchase'] = data['age_at_purchase'].apply(np.floor)\n",
    "\n",
    "# create columns for the three main categories age groups\n",
    "data['age_18_35'] = 0\n",
    "data['age_36_55'] = 0\n",
    "data['age_over_55'] = 0\n",
    "\n",
    "# flags for the main age groups\n",
    "data.loc[(data['age_at_purchase'] >= 18) & (data['age_at_purchase'] <= 35), 'age_18_35'] = 1\n",
    "data.loc[(data['age_at_purchase'] >= 36) & (data['age_at_purchase'] <= 55), 'age_36_55'] = 1\n",
    "data.loc[data['age_at_purchase'] > 55, 'age_over_55'] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['days_since_first_purchase'] = (retrieval_date - data['min_purchase_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get month of year and day of week of start_date and min_purchase_date\n",
    "data['start_month_num'] = data['start_date'].dt.month\n",
    "data['start_dayofweek_num'] = data['start_date'].dt.dayofweek\n",
    "\n",
    "data['purchase_month_num'] =  data['min_purchase_date'].dt.month\n",
    "\n",
    "weekend_days = [4, 5, 6]\n",
    "data['is_weekend_performance'] = data['start_dayofweek_num'].isin(weekend_days).astype(int)\n",
    "\n",
    "month_map = {\n",
    "    1: 'january',\n",
    "    2: 'february',\n",
    "    3: 'march',\n",
    "    4: 'april',\n",
    "    5: 'may',\n",
    "    6: 'june',\n",
    "    7: 'july',\n",
    "    8: 'august',\n",
    "    9: 'september',\n",
    "    10: 'october',\n",
    "    11: 'november',\n",
    "    12: 'december'\n",
    "}\n",
    "\n",
    "data['start_month_name'] = data['start_month_num'].map(month_map)\n",
    "\n",
    "data = pd.get_dummies(data, columns=['start_month_name'], prefix='start_month', dtype=int, dummy_na=False)\n",
    "\n",
    "max_val_month = 12\n",
    "data['purchase_month_sin'] = np.sin(2 * np.pi * data['purchase_month_num'] / max_val_month)\n",
    "data['purchase_month_cos'] = np.cos(2 * np.pi * data['purchase_month_num'] / max_val_month)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummies just for male and female\n",
    "data['gender_male'] = (data['gender'] == 'male').astype(int)\n",
    "data['gender_female'] = (data['gender'] == 'female').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first start_date of each production should be 1 since it is the first performance of the production, indicating premiere status\n",
    "# flirt should remain 0 \n",
    "data['is_premiere'] = data.groupby('production')['start_date'].transform('min')\n",
    "data['is_premiere'] = (data['start_date'] == data['is_premiere']).astype(int)\n",
    "data.loc[data['production'].str.contains(' flirt', case=False, na=False), 'is_premiere'] = 0\n",
    "\n",
    "data['is_flirt'] = data['production'].str.contains(' flirt', case=False, na=False).astype(int)\n",
    "\n",
    "# remove ' flirt' from production name\n",
    "data['production'] = data['production'].str.replace(' flirt', '', case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the location from cities, municipalities, and then countries (in that order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from survival.utils import geonames_cleaner\n",
    "geonames_cleaner(data, ['city', 'municipality'])\n",
    "geonames_cleaner(nl, ['name'])\n",
    "geonames_cleaner(hh, ['municipality'])\n",
    "geonames_cleaner(gemeenten, ['oude_naam', 'nieuwe_naam'])\n",
    "geonames_cleaner(cities, ['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# known dutch locations\n",
    "known_nl_cities = set(nl['name'].dropna().unique()) \\\n",
    "    .union(set(cities.loc[cities['country_code'] == 'nl', 'name'].dropna().unique())) \\\n",
    "    .union(set(data.loc[data['country_code'] == 'nl', 'city'].dropna().unique()))\n",
    "known_nl_municipalities = set(hh['municipality'].dropna().unique()) \\\n",
    "    .union(set(gemeenten['nieuwe_naam'].dropna().unique())) \\\n",
    "    .union(set(data.loc[data['country_code'] == 'nl', 'municipality'].dropna().unique()))\n",
    "\n",
    "# nan country_codes if city or municipality is known dutch\n",
    "nan_cc_city_nl_mask = data['country_code'].isna() & data['city'].isin(known_nl_cities)\n",
    "data.loc[nan_cc_city_nl_mask, 'country_code'] = 'nl'\n",
    "nan_cc_municipality_nl_mask = data['country_code'].isna() & data['municipality'].isin(known_nl_municipalities)\n",
    "data.loc[nan_cc_municipality_nl_mask, 'country_code'] = 'nl'\n",
    "\n",
    "# override existing non-'nl' country_codes\n",
    "city_is_nl = data['city'].isin(known_nl_cities)\n",
    "municipality_is_nl = data['municipality'].isin(known_nl_municipalities)\n",
    "city_is_nan = data['city'].isna()\n",
    "municipality_is_nan = data['municipality'].isna()\n",
    "\n",
    "override_cond = (city_is_nl & municipality_is_nl) | \\\n",
    "                (city_is_nl & municipality_is_nan) | \\\n",
    "                (city_is_nan & municipality_is_nl)\n",
    "\n",
    "actual_override_mask = (data['country_code'].notna() & (data['country_code'] != 'nl')) & override_cond\n",
    "data.loc[actual_override_mask, 'country_code'] = 'nl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_nl'] = (data['country_code'] == 'nl').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postal codes longitude and latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data['postcode'], remove all non-numeric characters\n",
    "data['postalcode'] = data['postalcode'].astype(str).str.replace(r'[^0-9]', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only a select amount digits of the postalcode\n",
    "data['pc_4'] = data['postalcode'].str[:4]\n",
    "data['pc_3'] = data['postalcode'].str[:3]\n",
    "data['pc_2'] = data['postalcode'].str[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lookup table\n",
    "lookup_table = postalcode[['pc_4', 'latitude', 'longitude']].copy()\n",
    "lookup_table.rename(columns={'latitude': 'latitude_pc_ref', 'longitude': 'longitude_pc_ref'}, inplace=True) # Rename to avoid clash\n",
    "lookup_table['pc_4'] = lookup_table['pc_4'].astype(str)\n",
    "lookup_table.drop_duplicates(subset=['pc_4'], keep='first', inplace=True)\n",
    "\n",
    "# 'pc_4' is string\n",
    "data['pc_4'] = data['pc_4'].astype(str)\n",
    "\n",
    "# preserve lat/lon if they exist, before merge\n",
    "if 'latitude' in data.columns:\n",
    "    data.rename(columns={'latitude': 'latitude_original', 'longitude': 'longitude_original'}, inplace=True)\n",
    "\n",
    "# merge to get PC4 coordinates\n",
    "data = pd.merge(\n",
    "    data,\n",
    "    lookup_table,\n",
    "    on='pc_4',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# final 'latitude' and 'longitude' columns with hierarchy\n",
    "# Initialize with NaNs\n",
    "data['latitude'] = np.nan\n",
    "data['longitude'] = np.nan\n",
    "\n",
    "# condition for using PC4 derived coordinates (dutch and successfully looked up)\n",
    "dutch_pc4_success_mask = (data['country_code'] == 'nl') & data['latitude_pc_ref'].notna()\n",
    "\n",
    "# populate with PC4 derived coordinates\n",
    "data.loc[dutch_pc4_success_mask, 'latitude'] = data.loc[dutch_pc4_success_mask, 'latitude_pc_ref']\n",
    "data.loc[dutch_pc4_success_mask, 'longitude'] = data.loc[dutch_pc4_success_mask, 'longitude_pc_ref']\n",
    "\n",
    "# fallback to original coordinates if PC4-derived are nan AND original exist\n",
    "if 'latitude_original' in data.columns:\n",
    "    fallback_mask = data['latitude'].isna() & data['latitude_original'].notna()\n",
    "    data.loc[fallback_mask, 'latitude'] = data.loc[fallback_mask, 'latitude_original']\n",
    "    data.loc[fallback_mask, 'longitude'] = data.loc[fallback_mask, 'longitude_original']\n",
    "\n",
    "# drop intermediate/original columns\n",
    "cols_to_drop = ['latitude_pc_ref', 'longitude_pc_ref']\n",
    "if 'latitude_original' in data.columns:\n",
    "    cols_to_drop.extend(['latitude_original', 'longitude_original'])\n",
    "data.drop(columns=cols_to_drop, errors='ignore', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cities longitude and latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city lookup table from 'nl' dataset\n",
    "# cityname as key\n",
    "city_lookup = nl[['name', 'latitude', 'longitude']].copy()\n",
    "city_lookup.rename(columns={'latitude': 'city_lat_ref', 'longitude': 'city_lon_ref'}, inplace=True)\n",
    "city_lookup.drop_duplicates(subset=['name'], keep='first', inplace=True) \n",
    "\n",
    "# merge city coordinates into 'data' (temporary suffixed cols)\n",
    "# dont overwrite existing lat/lon\n",
    "data = pd.merge(\n",
    "    data,\n",
    "    city_lookup,\n",
    "    left_on='city',\n",
    "    right_on='name',\n",
    "    how='left',\n",
    "    suffixes=('', '_city_lookup_temp')\n",
    ").drop(columns=['name'], errors='ignore')\n",
    "\n",
    "# names of newly merged columns\n",
    "lat_col_from_city_lookup = 'city_lat_ref_city_lookup_temp' if 'city_lat_ref_city_lookup_temp' in data.columns else 'city_lat_ref'\n",
    "lon_col_from_city_lookup = 'city_lon_ref_city_lookup_temp' if 'city_lon_ref_city_lookup_temp' in data.columns else 'city_lon_ref'\n",
    "\n",
    "\n",
    "# conditionally fill 'latitude'/'longitude' if they are still nan\n",
    "#    AND the record is Dutch AND city lookup was successful\n",
    "fill_mask = (\n",
    "    data['latitude'].isna() &                 \n",
    "    (data['country_code'] == 'nl') &          \n",
    "    data[lat_col_from_city_lookup].notna() \n",
    ")\n",
    "\n",
    "data.loc[fill_mask, 'latitude'] = data.loc[fill_mask, lat_col_from_city_lookup]\n",
    "data.loc[fill_mask, 'longitude'] = data.loc[fill_mask, lon_col_from_city_lookup]\n",
    "\n",
    "# remove temporary merged columns\n",
    "cols_to_drop_temp = [lat_col_from_city_lookup, lon_col_from_city_lookup]\n",
    "data.drop(columns=cols_to_drop_temp, errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_names_dict = {}\n",
    "for _, row in nl.iterrows(): # nl should ideally have unique cleaned 'name'\n",
    "    if isinstance(row['alternatenames'], str):\n",
    "        main_name = row['name']\n",
    "        for alt_name_raw in row['alternatenames'].split(','):\n",
    "            cleaned_alt = alt_name_raw.strip() \n",
    "            if cleaned_alt:\n",
    "                alt_names_dict[cleaned_alt] = main_name\n",
    "\n",
    "# identify dutch cities in data still needing coordinates\n",
    "missing_coords_mask = (data['country_code'] == 'nl') & data['latitude'].isna() & data['city'].notna()\n",
    "cities_to_fill = data.loc[missing_coords_mask, 'city'].unique()\n",
    "\n",
    "# fill coordinates using the alt_names_dict and city_lookup\n",
    "for city_in_data in cities_to_fill:\n",
    "    # cleaned_city_for_lookup = geonames_cleaner_function(city_in_data) # If data['city'] needs cleaning to match dict keys\n",
    "    cleaned_city_for_lookup = city_in_data\n",
    "\n",
    "    if cleaned_city_for_lookup in alt_names_dict:\n",
    "        main_name = alt_names_dict[cleaned_city_for_lookup]\n",
    "        coords_row = city_lookup[city_lookup['name'] == main_name]\n",
    "\n",
    "        if not coords_row.empty:\n",
    "            update_mask = (data['country_code'] == 'nl') & \\\n",
    "                          (data['city'] == city_in_data) & \\\n",
    "                          data['latitude'].isna()\n",
    "            data.loc[update_mask, 'latitude'] = coords_row['city_lat_ref'].iloc[0]\n",
    "            data.loc[update_mask, 'longitude'] = coords_row['city_lon_ref'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check remaining missing cities\n",
    "still_missing = data[\n",
    "    (data['country_code'] == 'nl') & \n",
    "    (data['longitude'].isna()) & \n",
    "    (data['city'].notna())\n",
    "]['city'].unique()\n",
    "\n",
    "print(f\"num of cities still missing: {len(still_missing)}\")\n",
    "print(\"still missing cities:\", still_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Municipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dutch municipality coordinate lookup from 'nl'\n",
    "nl_municipalities_src = nl[\n",
    "    (nl['alternatenames'].str.contains(' munici|gemeente', case=False, na=False)) |\n",
    "    (nl['name'].str.contains('gemeente', case=False, na=False)) |\n",
    "    (nl['feature_code'] == 'adm2')\n",
    "].copy() # use .copy()\n",
    "\n",
    "nl_municipalities_src['name'] = nl_municipalities_src['name'].str.replace('^gemeente ', '', regex=True).str.strip()\n",
    "nl_municipalities_src.loc[nl_municipalities_src['name'] == 'westvoorne', 'name'] = 'voorne aan zee'\n",
    "\n",
    "municipality_lookup = nl_municipalities_src.sort_values('feature_code') \\\n",
    "    .drop_duplicates(subset=['name'], keep='first') \\\n",
    "    [['name', 'latitude', 'longitude']] \\\n",
    "    .rename(columns={'latitude': 'mun_lat_ref', 'longitude': 'mun_lon_ref'})\n",
    "\n",
    "# merge municipality coordinates (into temporary columns)\n",
    "data = pd.merge(\n",
    "    data,\n",
    "    municipality_lookup,\n",
    "    left_on='municipality',\n",
    "    right_on='name',\n",
    "    how='left'\n",
    ").drop(columns=['name'], errors='ignore')\n",
    "\n",
    "# conditionally fill main 'latitude'/'longitude'\n",
    "fill_mask = (data['latitude'].isna()) & \\\n",
    "            (data['country_code'] == 'nl') & \\\n",
    "            data['mun_lat_ref'].notna()\n",
    "\n",
    "data.loc[fill_mask, 'latitude'] = data.loc[fill_mask, 'mun_lat_ref']\n",
    "data.loc[fill_mask, 'longitude'] = data.loc[fill_mask, 'mun_lon_ref']\n",
    "\n",
    "# remove temporary columns\n",
    "data.drop(columns=['mun_lat_ref', 'mun_lon_ref'], errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remaining missing coordinates\n",
    "still_missing = data[\n",
    "    (data['longitude'].isna()) & \n",
    "    (data['city'].notna()) &\n",
    "    (data['country_code'] == 'nl')\n",
    "]\n",
    "print(f\"Number of records still missing coordinates: {len(still_missing)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcli = countries[countries['feature_code'] == 'pcli'].drop(columns=['name', 'feature_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask records that need country coordinates\n",
    "missing_coords_mask = (data['longitude'].isna()) & (data['country_code'] != 'nl')\n",
    "\n",
    "# country coordinates mapping\n",
    "country_coords = pcli.set_index('country_code')[['latitude', 'longitude']]\n",
    "\n",
    "# update latitude and longitude\n",
    "data.loc[missing_coords_mask, 'latitude'] = (\n",
    "    data.loc[missing_coords_mask, 'country_code']\n",
    "    .map(country_coords['latitude'])\n",
    ")\n",
    "data.loc[missing_coords_mask, 'longitude'] = (\n",
    "    data.loc[missing_coords_mask, 'country_code']\n",
    "    .map(country_coords['longitude'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the missing countries by checking the feature_code starting with 'pcl' in countries and add long and lat to data\n",
    "missing_cc = data.loc[(data['longitude'].isna()) & (data['country_code'] != 'nl'), 'country_code'].unique()\n",
    "\n",
    "missing_countries = countries[\n",
    "\tcountries['country_code'].isin(missing_cc) & \n",
    "\tcountries['feature_code'].str.startswith('pcl', na=False)\n",
    "].drop_duplicates(subset='country_code')\n",
    "\n",
    "data.loc[\n",
    "\t(data['longitude'].isna()) & (data['country_code'].notna()), \n",
    "\t'longitude'\n",
    "] = data.loc[\n",
    "\t(data['longitude'].isna()) & (data['country_code'].notna()), \n",
    "\t'country_code'\n",
    "].map(missing_countries.set_index('country_code')['longitude'])\n",
    "\n",
    "data.loc[\n",
    "\t(data['latitude'].isna()) & (data['country_code'].notna()), \n",
    "\t'latitude'\n",
    "] = data.loc[\n",
    "\t(data['latitude'].isna()) & (data['country_code'].notna()), \n",
    "\t'country_code'\n",
    "].map(missing_countries.set_index('country_code')['latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually look up the lat and long of american samoa and namibia\n",
    "american_samoa = (-14.23377, -169.47767)\n",
    "namibia = (-22.00000, 17.00000)\n",
    "\n",
    "# update the lat and long of american samoa and namibia\n",
    "data.loc[data['country_code'] == 'as', 'latitude'] = american_samoa[0]\n",
    "data.loc[data['country_code'] == 'as', 'longitude'] = american_samoa[1]\n",
    "\n",
    "data.loc[data['country_code'] == 'na', 'latitude'] = namibia[0]\n",
    "data.loc[data['country_code'] == 'na', 'longitude'] = namibia[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the distance of known longitude and latitude to nob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every lan and long, calculate the distance to the nob\n",
    "nob = (52.367608492466346, 4.901889580039182)\n",
    "\n",
    "data['distance_to_nob'] = data.apply(\n",
    "    lambda row: geodesic(nob, (row['latitude'], row['longitude'])).kilometers\n",
    "    if pd.notnull(row['latitude']) and pd.notnull(row['longitude']) else None,\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the income from hh and match with data municipality\n",
    "data = data.merge(hh, left_on='municipality', right_on='municipality', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set for imputation is a 365 days minus the max purchase date\n",
    "cutoff = retrieval_date - pd.Timedelta(days=365)\n",
    "\n",
    "# ensure all date columns are datetime type\n",
    "data['start_date'] = pd.to_datetime(data['start_date'])\n",
    "data['purchase_date'] = pd.to_datetime(data['purchase_date'])\n",
    "data['next_purchase_date'] = pd.to_datetime(data['next_purchase_date'])\n",
    "\n",
    "# extract just the date part of start_date for calculations\n",
    "start_date_only = data['start_date'].dt.date\n",
    "\n",
    "# calculate the end of observation window from performance date\n",
    "observation_end = pd.to_datetime(start_date_only) + pd.Timedelta(days=365)\n",
    "\n",
    "# for train, cap observation window at cutoff date\n",
    "training_mask = data['start_date'] < cutoff\n",
    "observation_end[training_mask] = observation_end[training_mask].clip(upper=cutoff)\n",
    "\n",
    "# for test, cap observation window at retrieval_date\n",
    "test_mask = data['start_date'] >= cutoff\n",
    "observation_end[test_mask] = observation_end[test_mask].clip(upper=retrieval_date)\n",
    "\n",
    "# event column\n",
    "data['event'] = 0\n",
    "\n",
    "# if next purchase is within observation window\n",
    "has_next_purchase = data['next_purchase_date'].notna()\n",
    "within_window = has_next_purchase & (data['next_purchase_date'] <= observation_end)\n",
    "data.loc[within_window, 'event'] = 1\n",
    "\n",
    "# calculate time using date-only version of start_date\n",
    "# for events: time from performance date to next purchase\n",
    "mask_event = data['event'] == 1\n",
    "data.loc[mask_event, 'time'] = (\n",
    "    data.loc[mask_event, 'next_purchase_date'] - pd.to_datetime(start_date_only[mask_event])\n",
    ").dt.days\n",
    "\n",
    "# for censored: time from performance date to end of observation window\n",
    "mask_censored = data['event'] == 0\n",
    "data.loc[mask_censored, 'time'] = (\n",
    "    observation_end[mask_censored] - pd.to_datetime(start_date_only[mask_censored])\n",
    ").dt.days\n",
    "\n",
    "# ensure time doesnt exceed 365 days and is not negative\n",
    "data['time'] = data['time'].clip(lower=0, upper=365)\n",
    "\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_train_set = data[data['start_date'] < cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary flag to indicate whether distance to nob is known\n",
    "data['distance_imputed'] = data['distance_to_nob'].isna().astype(int) \n",
    "\n",
    "# fill missing distances with the median of the distance of country_code nl of the impute set\n",
    "data['distance_to_nob'] = data['distance_to_nob'].fillna(impute_train_set[impute_train_set['country_code'] == 'nl']['distance_to_nob'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBEFORE IMPUTATION:\")\n",
    "train_missing_pct = impute_train_set['median_income'].isna().mean() * 100\n",
    "test_missing_pct = data[data['start_date'] >= cutoff]['median_income'].isna().mean() * 100\n",
    "print(f\"- Train data missing income: {train_missing_pct:.2f}%\")\n",
    "print(f\"- Test data missing income: {test_missing_pct:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_muni = impute_train_set['city'].notna().mean() * 100\n",
    "test_muni = data[data['start_date'] >= cutoff]['city'].notna().mean() * 100\n",
    "print(f\"- train data with city: {train_muni:.2f}%\")\n",
    "print(f\"- test data with city: {test_muni:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize imputation flags\n",
    "\n",
    "data['foreign_income_imputed'] = 0\n",
    "\n",
    "# median values for fallback options using train only\n",
    "nl_median = impute_train_set.loc[impute_train_set['country_code'] == 'nl', 'median_income'].median()\n",
    "foreign_median = impute_train_set.loc[(impute_train_set['country_code'] != 'nl') & \n",
    "                                    impute_train_set['country_code'].notna(), 'median_income'].median()\n",
    "\n",
    "# country income lookup dictionary\n",
    "country_income_map = income.set_index('country_code')['median_income'].to_dict()\n",
    "\n",
    "# imputation for Dutch records with coordinates\n",
    "# create and train the imputer using train\n",
    "nl_train_data = impute_train_set[\n",
    "    (impute_train_set['country_code'] == 'nl') & \n",
    "    impute_train_set['latitude'].notna() & \n",
    "    impute_train_set['longitude'].notna() &\n",
    "    impute_train_set['median_income'].notna()  # only known values\n",
    "]\n",
    "\n",
    "# features and target\n",
    "features = ['latitude', 'longitude', 'distance_to_nob']\n",
    "target = 'median_income'\n",
    "\n",
    "# fit the imputer (only if theres train data)\n",
    "if not nl_train_data.empty:\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=RandomForestRegressor(\n",
    "            n_estimators=50, \n",
    "            random_state=42,\n",
    "            max_depth=10\n",
    "        ),\n",
    "        random_state=42,\n",
    "        max_iter=10,\n",
    "        initial_strategy='mean'\n",
    "    )\n",
    "    \n",
    "    # fit\n",
    "    imputer.fit(nl_train_data[features + [target]])\n",
    "    \n",
    "    # apply to all data that needs imputation\n",
    "    nl_missing = data[\n",
    "        (data['country_code'] == 'nl') & \n",
    "        data['latitude'].notna() & \n",
    "        data['longitude'].notna() &\n",
    "        data['median_income'].isna()\n",
    "    ]\n",
    "    \n",
    "    if not nl_missing.empty:\n",
    "        missing_indices = nl_missing.index\n",
    "        X_missing = nl_missing[features + [target]].copy()\n",
    "        \n",
    "        # transform using the trained imputer\n",
    "        X_imputed = imputer.transform(X_missing)\n",
    "        \n",
    "        # update values\n",
    "        data.loc[missing_indices, 'median_income'] = X_imputed.values[:, -1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# country-specific lookup for non-dutch records\n",
    "non_nl_mask = (\n",
    "    data['country_code'].notna() & \n",
    "    (data['country_code'] != 'nl') & \n",
    "    data['median_income'].isna()\n",
    ")\n",
    "\n",
    "# original state to check which ones get filled\n",
    "original_non_nl_missing = data.loc[non_nl_mask, 'median_income'].copy()\n",
    "\n",
    "# apply country-specific values\n",
    "data.loc[non_nl_mask, 'median_income'] = data.loc[non_nl_mask, 'country_code'].map(country_income_map)\n",
    "\n",
    "# flag successfully imputed values\n",
    "actually_imputed_mask = non_nl_mask & data['median_income'].notna() & original_non_nl_missing.isna()\n",
    "data.loc[actually_imputed_mask, 'foreign_income_imputed'] = 1\n",
    "\n",
    "# dutch median for remaining dutch records\n",
    "remaining_nl_mask = (data['country_code'] == 'nl') & data['median_income'].isna()\n",
    "if pd.notna(nl_median) and remaining_nl_mask.any():\n",
    "    data.loc[remaining_nl_mask, 'median_income'] = nl_median\n",
    "\n",
    "\n",
    "# dutch median for records with unknown country code\n",
    "unknown_country_mask = data['country_code'].isna() & data['median_income'].isna()\n",
    "if pd.notna(nl_median) and unknown_country_mask.any():\n",
    "    data.loc[unknown_country_mask, 'median_income'] = nl_median\n",
    "\n",
    "\n",
    "# fill remaining missing values with dutch median as final fallback\n",
    "still_missing_mask = data['median_income'].isna()\n",
    "if pd.notna(nl_median) and still_missing_mask.any():\n",
    "    data.loc[still_missing_mask, 'median_income'] = nl_median\n",
    "\n",
    "print(f\"foreign records imputed: {data['foreign_income_imputed'].sum()}\")\n",
    "print(f\"records still missing income: {data['median_income'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # After running validation\n",
    "# results_df = validate_imputation(nl_data, features, target)\n",
    "\n",
    "# # Calculate confidence intervals\n",
    "# for metric in ['RMSE', 'MAE', 'R2', 'NRMSE']:\n",
    "#    mean = results_df[metric].mean()\n",
    "#    std = results_df[metric].std()\n",
    "#    ci = std * 1.96  # 95% ci\n",
    "#    print(f\"\\n{metric}:\")\n",
    "#    print(f\"Mean: {mean:.4f}  {ci:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process postal code information for all records\n",
    "data['pc_4'] = data['pc_4'].astype(str)\n",
    "data['pc_3'] = data['pc_4'].str[:3]  \n",
    "data['pc_2'] = data['pc_4'].str[:2] \n",
    "\n",
    "# valid records for detailed location binning\n",
    "valid_records = (\n",
    "    (data['country_code'] == 'nl') & \n",
    "    (data['distance_imputed'] == 0) & \n",
    "    (data['pc_4'].str.len() == 4) & \n",
    "    (data['pc_4'].str.isdigit()) &\n",
    "    (~data['pc_4'].str.startswith('0'))\n",
    ")\n",
    "\n",
    "# location_bin column with default value\n",
    "data['location_bin'] = 'Other'  # default\n",
    "\n",
    "# apply specific location binning for valid dutch records\n",
    "# amsterdam postal codes mpre granular\n",
    "def apply_location_bin(row):\n",
    "    try:\n",
    "        pc3 = int(row['pc_3'])\n",
    "        # amsterdam postal codes\n",
    "        if 100 <= pc3 <= 111:\n",
    "            return f\"AMS_PC3_{row['pc_3']}\"\n",
    "        else:\n",
    "            # other Dutch postal codes with valid format\n",
    "            return f\"NL_PC2_{row['pc_2']}\"\n",
    "    except (ValueError, TypeError):\n",
    "        return 'Invalid_PC4'  # For any conversion errors\n",
    "\n",
    "# process records that meet criteria\n",
    "data.loc[valid_records, 'location_bin'] = data.loc[valid_records].apply(apply_location_bin, axis=1)\n",
    "\n",
    "# data for target encoding\n",
    "training_data = data[data['start_date'] < cutoff].copy()\n",
    "training_data = training_data.dropna(subset=['location_bin', 'event'])\n",
    "\n",
    "# fit the target encoder\n",
    "target_encoder = TargetEncoder(\n",
    "    cols=['location_bin'],\n",
    "    handle_unknown='value',\n",
    "    handle_missing='value',\n",
    "    smoothing=20.0\n",
    ")\n",
    "\n",
    "target_encoder.fit(training_data[['location_bin']], training_data['event'])\n",
    "\n",
    "# apply encoding\n",
    "data['location_bin_encoded_rate'] = target_encoder.transform(data[['location_bin']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not exist post-covid\n",
    "data = data.drop(columns=['tickets_type_migration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opera 1, ballet 0\n",
    "data = data.rename(columns={\n",
    "    'opera': 'artform'}).drop(columns=['ballet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant seasons\n",
    "data = data.loc[data['season'].isin(['2022_2023', '2023_2024', '2024_2025', '2025_2026'])].copy()\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_features = [\n",
    "    # Order metrics\n",
    "    'order_size', \n",
    "    'total_order_value',\n",
    "    #'avg_order_value',\n",
    "    'artform',\n",
    "    'best_rank_per_purchase',\n",
    "    'lead_days',\n",
    "    \n",
    "    # performance metrics\n",
    "    'perf_sales_cumulative', # maybe remove\n",
    "    'prod_sales_cumulative', \n",
    "    'daily_perf_sales', \n",
    "    'daily_prod_sales', \n",
    "    #'final_perf_sales', \n",
    "    'perf_sales_percentage',\n",
    "    'perf_free_tickets_cumulative', \n",
    "    'perf_operational_discount_cumulative',\n",
    "    \n",
    "    # timing features\n",
    "    'days_since_sale_start', 'sale_period_duration_days', 'days_left_in_season',\n",
    "    \n",
    "    # audience type and occupancy\n",
    "    'subscriber_percentage', 'regular_percentage', 'occupancy_rate',\n",
    "    \n",
    "    # ticket types\n",
    "    'tickets_type_artform_discount', 'tickets_type_regular',\n",
    "    'tickets_type_special_discount', 'tickets_type_stadspas', 'tickets_type_student',\n",
    "    'tickets_type_youth_16', 'tickets_type_youth_35',\n",
    "    \n",
    "    # customer demographics\n",
    "    'age_18_35', 'age_36_55', 'age_over_55', 'gender_male', 'gender_female', 'is_nl',\n",
    "    \n",
    "    # performance attributes\n",
    "    'is_weekend_performance', 'is_premiere', 'is_flirt',\n",
    "    \n",
    "    # seasonality \n",
    "    'start_month_april', 'start_month_december', 'start_month_february',\n",
    "    'start_month_january', 'start_month_july', 'start_month_june',\n",
    "    'start_month_march', 'start_month_may', 'start_month_november',\n",
    "    'start_month_october', 'start_month_september',\n",
    "    'purchase_month_sin', 'purchase_month_cos',\n",
    "    \n",
    "    # location data\n",
    "    'distance_to_nob', 'median_income', 'distance_imputed',\n",
    "    'foreign_income_imputed', 'location_bin_encoded_rate',\n",
    "\n",
    "    # time event\n",
    "    'time', 'event'\n",
    "]\n",
    "\n",
    "start = ['start_date']\n",
    "\n",
    "# scale certain features\n",
    "features_to_scale = [\n",
    "    'order_size', \n",
    "    'total_order_value', \n",
    "    #'avg_order_value',\n",
    "    'lead_days',\n",
    "    'best_rank_per_purchase', \n",
    "    'perf_sales_cumulative', \n",
    "    'prod_sales_cumulative',\n",
    "    'daily_perf_sales', \n",
    "    'daily_prod_sales', \n",
    "    #'final_perf_sales',\n",
    "    'perf_free_tickets_cumulative', \n",
    "    'perf_operational_discount_cumulative',\n",
    "    'days_since_sale_start', \n",
    "    'sale_period_duration_days', \n",
    "    'days_left_in_season',\n",
    "    'distance_to_nob', \n",
    "    'median_income', \n",
    "    'tickets_type_artform_discount', \n",
    "    'tickets_type_regular',\n",
    "    'tickets_type_special_discount', \n",
    "    'tickets_type_stadspas', \n",
    "    'tickets_type_student',\n",
    "    'tickets_type_youth_16', \n",
    "    'tickets_type_youth_35'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for scaling\n",
    "data = data[standard_features + start].copy()\n",
    "\n",
    "# split at cutoff\n",
    "train_data = data[data['start_date'] < cutoff].copy()\n",
    "eval_data = data[data['start_date'] >= cutoff].copy()\n",
    "\n",
    "# fit scaler \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data[features_to_scale])\n",
    "\n",
    "# transform train and eval\n",
    "train_data[features_to_scale] = scaler.transform(train_data[features_to_scale])\n",
    "eval_data[features_to_scale] = scaler.transform(eval_data[features_to_scale])\n",
    "\n",
    "train_data = train_data.drop(columns=['start_date'])\n",
    "eval_data = eval_data.drop(columns=['start_date'])\n",
    "\n",
    "# recombine back to full dataset if needed\n",
    "data = pd.concat([train_data, eval_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hyperparameter tuning splits from train_data\n",
    "test, val = train_test_split(\n",
    "    eval_data, \n",
    "    test_size=0.5, \n",
    "    stratify=eval_data['event'], \n",
    "    random_state=1212\n",
    ")\n",
    "\n",
    "# reset indices\n",
    "test = test.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "\n",
    "# remove target variables if not in the feature lists\n",
    "standard_features = [f for f in standard_features if f not in ['time', 'event']]\n",
    "\n",
    "\n",
    "# Define export path\n",
    "export_path = '../../data/processed/'\n",
    "\n",
    "# convert numeric to float32 \n",
    "numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "for df in [train_data, eval_data, test, val]:\n",
    "    df[numeric_cols] = df[numeric_cols].astype('float32')\n",
    "\n",
    "# export complete datasets\n",
    "data.to_parquet(f'{export_path}complete_dataset.parquet', index=False)\n",
    "\n",
    "# export features and targets separately\n",
    "# train\n",
    "train_data[standard_features + ['event', 'time']].to_parquet(f'{export_path}exploratory_data.parquet', index=False)\n",
    "train_data[standard_features].to_parquet(f'{export_path}train_standard_features.parquet', index=False)\n",
    "train_data[['time', 'event']].to_parquet(f'{export_path}train_targets.parquet', index=False)\n",
    "\n",
    "# test\n",
    "test[standard_features].to_parquet(f'{export_path}test_standard_features.parquet', index=False)\n",
    "test[['time', 'event']].to_parquet(f'{export_path}test_targets.parquet', index=False)\n",
    "\n",
    "# validation\n",
    "val[standard_features].to_parquet(f'{export_path}val_standard_features.parquet', index=False)\n",
    "val[['time', 'event']].to_parquet(f'{export_path}val_targets.parquet', index=False)\n",
    "\n",
    "# export feature lists as JSON \n",
    "import json\n",
    "with open(f'{export_path}feature_lists.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'standard_features': standard_features,\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"exported {len(data)} total records\")\n",
    "print(f\"train: {len(train_data)} records\")\n",
    "print(f\"tandard features: {len(standard_features)}\")\n",
    "print(f\"test: {len(test)} records\")\n",
    "print(f\"val: {len(val)} records\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
